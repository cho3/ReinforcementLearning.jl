{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridWorld Example\n",
    "\n",
    "Let's see how to make this API work with GridWorld! This reinforcement learning API requires 3 things to be defined before we start running algorithms:\n",
    "\n",
    "+ BlackBoxModel: defines the problem--see below for an example!\n",
    "+ Policy: this is where your domain knowledge comes in--define action space and feature functions\n",
    "+ Solver: This is where the API takes over and you just specify what you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "include(joinpath(\"..\",\"src\",\"ReinforcementLearning.jl\"))\n",
    "using ReinforcementLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Black Box Model Functions\n",
    "\n",
    "The BlackBoxModel type requires the following things to be defined:\n",
    "+ `model`: a generic type that holds all your model parameters for a specific instance of your problem\n",
    "+ `init(model,rng)`: generate an initial state\n",
    "+ `observe(model,rng,state,action=None)`: return an observation based on your state (and action--this isn't quite ironed out yet)\n",
    "+ `next_state(model,rng,state,action)`: generate a next state given your state, action and problem parameterization\n",
    "+ `reward(model,rng,state,action)`: generate a reward based on your state and action and problem parameterization\n",
    "+ `isterminal(model,state,action)`: return a boolean of whether a state (and action) is terminal or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "using Interact\n",
    "\n",
    "typealias State Tuple{Int,Int}\n",
    "typealias Action Tuple{Int,Int}\n",
    "\n",
    "type GridWorldModel <: Model\n",
    "  W::Int\n",
    "  H::Int\n",
    "  p_other::Float64\n",
    "  reward_locs::Dict{State,Float64}\n",
    "  collide_cost::Float64\n",
    "    A::Array{Action,1}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init2(m::GridWorldModel,rng::AbstractRNG) = (rand(rng,1:m.W),rand(rng,1:m.H))\n",
    "init1(m::GridWorldModel,rng::AbstractRNG) = (1,1)\n",
    "\n",
    "isend1(rng::AbstractRNG,m::GridWorldModel,s::State,a::Action) = s == (m.W,m.H)\n",
    "isend2(rng::AbstractRNG,m::GridWorldModel,s::State,a::Action) = false\n",
    "\n",
    "function reward(rng::AbstractRNG,m::GridWorldModel,s::State,a::Action)\n",
    "    r = get(m.reward_locs,s,0.)\n",
    "  x_ = s[1] + a[1]\n",
    "  y_ = s[2] + a[2]\n",
    "\n",
    "  if (x_ < 1) || (x_ > m.W)\n",
    "    r += m.collide_cost\n",
    "  elseif (y_ < 1) || (x_ > m.H)\n",
    "    r += m.collide_cost\n",
    "  end\n",
    "    \n",
    "    r -= 0.1 #cost of living\n",
    "    \n",
    "  return r\n",
    "\n",
    "end\n",
    "\n",
    "function next(rng::AbstractRNG,m::GridWorldModel,s::State,a::Action)\n",
    "    A_other = setdiff(m.A,[a,(-1*a[1],-1*a[2])])\n",
    "\n",
    "  if rand(rng) < m.p_other\n",
    "    _a = A_other[rand(rng,1:length(A_other))]\n",
    "  else\n",
    "    _a = a\n",
    "  end\n",
    "  x_ = s[1] + _a[1]\n",
    "  y_ = s[2] + _a[2]\n",
    "\n",
    "  x_ = max(min(x_,m.W),1)\n",
    "  y_ = max(min(y_,m.H),1)\n",
    "\n",
    "  return (x_,y_)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we also implement some quality of life functions, such as an explicity one-hot feature function for each state-action pair, and a visualization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function generate_featurefunction(m::GridWorldModel,A::Array{Action,1})\n",
    "\n",
    "  nb_feat = m.W*m.H*length(A)\n",
    "  A_indices = [a=>i for (i,a) in enumerate(A)]\n",
    "  function feature_function(s::State,a::Action)\n",
    "    active_indices = [s[1]+m.W*(s[2]-1)+m.W*m.H*(A_indices[a]-1)]\n",
    "    phi = sparsevec(active_indices,ones(length(active_indices)),nb_feat)\n",
    "    return phi\n",
    "  end\n",
    "\n",
    "  return feature_function\n",
    "\n",
    "end\n",
    "\n",
    "function visualize(m::GridWorldModel,s::State,a::Action)\n",
    "  #base grid\n",
    "  for i = 1:m.W\n",
    "    for j = 1:m.H\n",
    "      val = get(m.reward_locs,(i,j),0.)\n",
    "      if val > 0\n",
    "        color = \"#31B404\"\n",
    "      elseif val < 0\n",
    "        color = \"#FF0000\"\n",
    "      else\n",
    "        color = \"#A4A4A4\"\n",
    "      end\n",
    "            fill([i;i+1;i+1;i],[j;j;j+1;j+1],color=color,edgecolor=\"#FFFFFF\")\n",
    "    end #j\n",
    "  end #i\n",
    "\n",
    "  #draw agent\n",
    "  agent_color = \"#0101DF\"\n",
    "  x = s[1] + 0.5\n",
    "  y = s[2] + 0.5\n",
    "  fill([x-0.5;x;x+0.5;x],[y;y-0.5;y;y+0.5],color=agent_color)\n",
    "  #draw direction\n",
    "  arrow(x,y,0.5*a[1],0.5*a[2],width=0.1,head_width=0.15,head_length=0.5)\n",
    "\n",
    "end\n",
    "\n",
    "function visualize(m::GridWorldModel,S::Array{State,1},A::Array{Action,1})\n",
    "  assert(length(S) == length(A))\n",
    "  f = figure()\n",
    "  @manipulate for i = 1:length(S); withfig(f) do\n",
    "    visualize(m,S[i],A[i]) end\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_A = Action[(0,0),(1,0),(-1,0),(0,1),(0,-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = 20\n",
    "H = 20\n",
    "p_other = 0.2\n",
    "reward_locs = Dict{State,Float64}((W,H)=>10.)\n",
    "collide_cost = -1.\n",
    "m = GridWorldModel(W,H,p_other,reward_locs,collide_cost,_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the BlackBoxModel type. Note that we do not include an observation function in the constructor--in this case, it uses a default identity observation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bbm = BlackBoxModel(m,init1,next,reward,isend1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Policy\n",
    "\n",
    "In general for a policy, we have to define an ActionSpace (which we require to be exactly or a subset of the true action space), and feature function, which maps the state into a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tile coding is provided (the API for tilecoding needs work, however) for a quick and dirty function approximator in the continuous domain. For concreteness/generality, we include a function `cast_mc_state`, which in the most general case, will convert whatever state representation you have into an array of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_function = generate_featurefunction(m,_A)\n",
    "A = DiscreteActionSpace(_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy = EpsilonGreedyPolicy(feature_function,A,rng=MersenneTwister(3234),eps=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose and Set up your Solver\n",
    "\n",
    "Currently, the following solvers are supported:\n",
    "+ Forgetful LSTD(\\lambda) / LS-SARSA (untested)\n",
    "+ SARSA(\\lamda) \n",
    "+ Q(\\lambda) (unimplemented)\n",
    "+ GQ(\\lambda) (unimplemented)\n",
    "+ Double Q learning (untested)\n",
    "+ Deterministic Policy Gradient (unimplemented)\n",
    "+ (Natural) Actor-Critic (unimplemented\n",
    "+ LSPI/Batch TD (untested)\n",
    "+ True Online TD\n",
    "\n",
    "We just ask that you know a-priori how big your feature vectors are to make initialization easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#there might be a smart way to stick this into a constructor, but for now...\n",
    "nb_features = length(policy.feature_function(bbm.state,domain(A)[1]))\n",
    "updater = ForgetfulLSTDParam(nb_features,alpha=0.001/3)\n",
    "#updater = SARSAParam(nb_features,lambda=0.7,init_method=\"unif_rand\",trace_type=\"replacing\")\n",
    "updater = TrueOnlineTDParam(nb_features,lambda=0.95,init_method=\"unif_rand\")\n",
    "#mem_size = 50\n",
    "#updater = LSPIParam(nb_features,mem_size,del=0.01,discount=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually set up the real solver\n",
    "\n",
    "Some random cool things supported include:\n",
    "+ minibatching\n",
    "+ experience replay\n",
    "+ adaptive learning rates, e.g.:\n",
    "    * momentum\n",
    "    * nesterov momentum\n",
    "    * rmsprop\n",
    "    * adagrad\n",
    "    * adadelta\n",
    "    * adam\n",
    "+ simulated annealing (probably shouldn't support this)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solver = Solver(updater,\n",
    "                lr=0.01,\n",
    "                nb_episodes=2000,\n",
    "                nb_timesteps=1000,\n",
    "                discount=0.99,\n",
    "                annealer=NullAnnealer(),\n",
    "                mb=NullMinibatcher(),\n",
    "                er=NullExperienceReplayer(),\n",
    "                display_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trained_policy = solve(solver,bbm,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Policy\n",
    "Basically just run a couple of simulations -- the simulator api is a subset of the stuff you see in solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sim = Simulator(discount=1.,nb_sim=50,nb_timesteps=1000,visualizer=visualize) #stuff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#returns average reward for now...\n",
    "bbm.init = init1\n",
    "R_avg = simulate(sim,bbm,trained_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize(m,sim.hist.S,sim.hist.A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have to call visualize externally. Currently getting the visualization to work two or three function calls in isn't quite working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.2",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
