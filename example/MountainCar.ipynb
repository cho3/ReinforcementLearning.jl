{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car Example\n",
    "\n",
    "Let's see how to make this API work with Mountain Car! This reinforcement learning API requires 3 things to be defined before we start running algorithms:\n",
    "\n",
    "+ BlackBoxModel: defines the problem--see below for an example!\n",
    "+ Policy: this is where your domain knowledge comes in--define action space and feature functions\n",
    "+ Solver: This is where the API takes over and you just specify what you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: include stuff\n",
    "import PyPlot: plot, xlabel, ylabel, subplot, suptitle #for solver.grandiloquent\n",
    "import StatsBase: sample, WeightVec #for policy.SoftmaxPolicy\n",
    "using HypothesisTests #for utils.test...\n",
    "\n",
    "typealias RealVector Union{Array{Float64,1},Array{Int,1},SparseMatrixCSC{Float64,Int},SparseMatrixCSC{Int,Int}}\n",
    "typealias RealMatrix Union{Array{Float64,2},Array{Int,2},SparseMatrixCSC{Float64,Int},SparseMatrixCSC{Int,Int}}\n",
    "dot(x::Array,y::SparseMatrixCSC) = (x'*y)[1]\n",
    "dot(x::SparseMatrixCSC,y::Array) = dot(y,x)\n",
    "\n",
    "import Base.assert\n",
    "function assert(expr,val,fn::Function= ==,varname::AbstractString=\"\")\n",
    "\tif !fn(expr,val)\n",
    "    error(\"Assertion failed: $varname : expected $val, got $expr\")\n",
    "\tend\n",
    "end\n",
    "\n",
    "abstract AnnealerParam\n",
    "abstract ExperienceReplayer\n",
    "abstract Minibatcher\n",
    "abstract UpdaterParam\n",
    "abstract ActionSpace\n",
    "abstract Policy\n",
    "abstract Model\n",
    "include(joinpath(\"..\",\"src\",\"BlackBoxModel.jl\"))\n",
    "include(joinpath(\"..\",\"src\",\"policy.jl\"))\n",
    "include(joinpath(\"..\",\"src\",\"simulator.jl\"))\n",
    "include(joinpath(\"..\",\"src\",\"learners.jl\"))\n",
    "include(joinpath(\"..\",\"src\",\"solvers\",\"__solvers.jl\"))\n",
    "\n",
    "include(joinpath(\"..\",\"src\",\"solve.jl\"))\n",
    "include(joinpath(\"..\",\"src\",\"utils.jl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Black Box Model Functions\n",
    "\n",
    "The BlackBoxModel type requires the following things to be defined:\n",
    "+ `model`: a generic type that holds all your model parameters for a specific instance of your problem\n",
    "+ `init(model,rng)`: generate an initial state\n",
    "+ `observe(model,rng,state,action=None)`: return an observation based on your state (and action--this isn't quite ironed out yet)\n",
    "+ `next_state(model,rng,state,action)`: generate a next state given your state, action and problem parameterization\n",
    "+ `reward(model,rng,state,action)`: generate a reward based on your state and action and problem parameterization\n",
    "+ `isterminal(model,state,action)`: return a boolean of whether a state (and action) is terminal or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type MtnCarModel <: Model\n",
    "    cost::Float64\n",
    "end\n",
    "MtnCarModel() = MtnCarModel(-1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init(m::MtnCarModel,rng::AbstractRNG) = [-0.5;0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function next_state(model::MtnCarModel,rng::AbstractRNG,s::Array{Float64,1},a::Float64)\n",
    "    x,v = s\n",
    "    v_ = v + a*0.001+cos(3*x)*-0.0025\n",
    "    v_ = max(min(0.07,v_),-0.07)\n",
    "    x_ = x+v_\n",
    "    #inelastic boundary\n",
    "    if x_ < -1.2\n",
    "        x_ = -1.2\n",
    "        v_ = 0.\n",
    "    end\n",
    "    return [x_;v_]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reward(m::MtnCarModel,rng::AbstractRNG,s::Array{Float64,1},a::Float64) = m.cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "isterminal(m::MtnCarModel,rng::AbstractRNG,s::Array{Float64,1},a::Float64) = s[1] >= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the BlackBoxModel type. Note that we do not include an observation function in the constructor--in this case, it uses a default identity observation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bbm = BlackBoxModel(MtnCarModel(),init,next_state,reward,isterminal) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Policy\n",
    "\n",
    "In general for a policy, we have to define an ActionSpace (which we require to be exactly or a subset of the true action space), and feature function, which maps the state into a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = DiscreteActionSpace([-1.;0.;1.]) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tile coding is provided (the API for tilecoding needs work, however) for a quick and dirty function approximator in the continuous domain. For concreteness/generality, we include a function `cast_mc_state`, which in the most general case, will convert whatever state representation you have into an array of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for concreteness, this function converts statespace to an array\n",
    "cast_mc_state(x)=x\n",
    "__feature_function_ = generate_tilecoder(10,10,A,[-1.2;-0.07],[0.6;0.07])\n",
    "feature_function(s,a)=__feature_function_(cast_mc_state(s),a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy = EpsilonGreedyPolicy(feature_function,A,rng=MersenneTwister(3234),eps=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose and Set up your Solver\n",
    "\n",
    "Currently, the following solvers are supported:\n",
    "+ Forgetful LSTD(\\lambda) / LS-SARSA (untested)\n",
    "+ SARSA(\\lamda) (untested)\n",
    "+ Q(\\lambda) (unimplemented)\n",
    "+ GQ(\\lambda) (unimplemented)\n",
    "+ Double Q learning (untested)\n",
    "+ Deterministic Policy Gradient (unimplemented)\n",
    "+ (Natural) Actor-Critic (unimplemented\n",
    "\n",
    "We just ask that you know a-priori how big your feature vectors are to make initialization easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#there might be a smart way to stick this into a constructor, but for now...\n",
    "nb_features = length(policy.feature_function(bbm.state,domain(A)[1]))\n",
    "updater = SARSAParam(nb_features,lambda=0.9,init_method=\"unif_rand\",trace_type=\"replacing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually set up the real solver\n",
    "\n",
    "Some random cool things supported include:\n",
    "+ minibatching\n",
    "+ experience replay\n",
    "+ adaptive learning rates, e.g.:\n",
    "    * momentum\n",
    "    * nesterov momentum\n",
    "    * rmsprop\n",
    "    * adagrad\n",
    "    * adadelta\n",
    "    * adam\n",
    "+ simulated annealing (probably shouldn't support this)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solver = Solver(updater,\n",
    "                lr=0.1,\n",
    "                nb_episodes=50,\n",
    "                nb_timesteps=6000,\n",
    "                discount=0.99,\n",
    "                annealer=NullAnnealer(),\n",
    "                mb=NullMinibatcher(),\n",
    "                er=NullExperienceReplayer(),\n",
    "                display_interval=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trained_policy = solve(solver,bbm,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Policy\n",
    "Basically just run a couple of simulations -- the simulator api is a subset of the stuff you see in solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sim = Simulator(discount=1.,nb_sim=100,nb_timesteps=2500) #stuff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#returns average reward for now...\n",
    "R_avg = simulate(sim,bbm,trained_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(updater.e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[weights(updater)'*feature_function(bbm.state,a) for a in domain(A)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.2",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
