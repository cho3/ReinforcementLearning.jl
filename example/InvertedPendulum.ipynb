{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridWorld Example\n",
    "\n",
    "Let's see how to make this API work with GridWorld! This reinforcement learning API requires 3 things to be defined before we start running algorithms:\n",
    "\n",
    "+ BlackBoxModel: defines the problem--see below for an example!\n",
    "+ Policy: this is where your domain knowledge comes in--define action space and feature functions\n",
    "+ Solver: This is where the API takes over and you just specify what you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "include(joinpath(\"..\",\"src\",\"ReinforcementLearning.jl\"))\n",
    "using ReinforcementLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Black Box Model Functions\n",
    "\n",
    "The BlackBoxModel type requires the following things to be defined:\n",
    "+ `model`: a generic type that holds all your model parameters for a specific instance of your problem\n",
    "+ `init(model,rng)`: generate an initial state\n",
    "+ `observe(model,rng,state,action=None)`: return an observation based on your state (and action--this isn't quite ironed out yet)\n",
    "+ `next_state(model,rng,state,action)`: generate a next state given your state, action and problem parameterization\n",
    "+ `reward(model,rng,state,action)`: generate a reward based on your state and action and problem parameterization\n",
    "+ `isterminal(model,state,action)`: return a boolean of whether a state (and action) is terminal or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "using Interact\n",
    "import Iterators.product\n",
    "\n",
    "typealias State Tuple{Float64,Float64}\n",
    "typealias Action Float64\n",
    "\n",
    "type InvertedPendulumModel <: Model\n",
    "    g::Float64\n",
    "    m::Float64\n",
    "    l::Float64\n",
    "    M::Float64\n",
    "    alpha::Float64\n",
    "    dt::Float64\n",
    "    function InvertedPendulumModel(;\n",
    "                                    g::Float64=9.81,\n",
    "                                    m::Float64=2.,\n",
    "                                    M::Float64=8.,\n",
    "                                    l::Float64=0.5,\n",
    "                                    dt::Float64=0.1)\n",
    "        self = new()\n",
    "        self.g = g\n",
    "        self.m = m\n",
    "        self.l = l\n",
    "        self.M = M\n",
    "        self.m = m\n",
    "        self.alpha = 1/(m+M)\n",
    "        self.dt = dt\n",
    "        return self\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#init2(m::GridWorldModel,rng::AbstractRNG) = (rand(rng,1:m.W),rand(rng,1:m.H))\n",
    "init(m::InvertedPendulumModel,rng::AbstractRNG) = ((rand(rng)-0.5)*0.1,(rand(rng)-0.5)*0.1)\n",
    "\n",
    "isend(rng::AbstractRNG,m::InvertedPendulumModel,s::State,a::Action) = false\n",
    "\n",
    "reward(rng::AbstractRNG,m::InvertedPendulumModel,s::State,a::Action) = abs(s[1]) < pi/2 ? 0.: -1.\n",
    "\n",
    "function dwdt(m::InvertedPendulumModel,th::Float64,w::Float64,u::Float64)\n",
    "    num = m.g*sin(th)-m.alpha*m.m*m.l*(w^2)*sin(2*th)*0.5 - m.alpha*cos(th)*u\n",
    "    den = (4/3)*m.l - m.alpha*m.l*(cos(th)^2)\n",
    "    return num/den\n",
    "end\n",
    "\n",
    "\n",
    "function rk45(m::InvertedPendulumModel,s::State,a::Action)\n",
    "    k1 = dwdt(m,s[1],s[2],a)\n",
    "    #something...\n",
    "end\n",
    "\n",
    "function euler(m::InvertedPendulumModel,s::State,a::Action)\n",
    "    alph = dwdt(m,s[1],s[2],a)\n",
    "    w_ = s[2] + alph*m.dt\n",
    "    th_ = s[1] + s[2]*m.dt + 0.5*alph*m.dt^2\n",
    "    if th_ > pi\n",
    "        th_ -= 2*pi\n",
    "    elseif th_ < -pi\n",
    "        th_ += 2*pi\n",
    "    end\n",
    "    return (th_,w_)\n",
    "end\n",
    "\n",
    "function next(rng::AbstractRNG,m::InvertedPendulumModel,s::State,a::Action)\n",
    "    a_offset = 20*(rand(rng)-0.5)\n",
    "    a_ = a + a_offset\n",
    "    \n",
    "    return euler(m,s,a_)\n",
    "    #something..\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we also implement some quality of life functions, such as an explicity one-hot feature function for each state-action pair, and a visualization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_th_bins = 100\n",
    "nb_w_bins = 100\n",
    "\n",
    "exemplars = product([-pi/4;0.;pi/4],[-1.;0;1.])\n",
    "\n",
    "dist(a::State,b::State) = norm([a[1]-b[1];a[2]-b[2]],2)\n",
    "\n",
    "radial_feature_function = generate_radial_basis(exemplars,1.,dist)\n",
    "\n",
    "function generate_joint_featurefunction(m::InvertedPendulumModel)\n",
    "    nb_feat = nb_th_bins*nb_w_bins\n",
    "  function feature_function(s::State)\n",
    "        active_indices = [ReinforcementLearning.bin(s[1],-pi,pi,nb_th_bins)+nb_th_bins*(ReinforcementLearning.bin(s[2],-1,1,nb_w_bins)-1)]\n",
    "    phi = sparsevec(active_indices,ones(length(active_indices)),nb_feat)\n",
    "    return phi\n",
    "  end\n",
    "  return feature_function\n",
    "end\n",
    "\n",
    "\n",
    "function generate_disjoint_featurefunction(m::InvertedPendulumModel)\n",
    "    nb_feat = nb_th_bins + nb_w_bins\n",
    "    function feature_function(s::State)\n",
    "        active_indices = [ReinforcementLearning.bin(s[1],-pi,pi,nb_th_bins);ReinforcementLearning.bin(s[2],-1,1,nb_w_bins)]\n",
    "        return sparsevec(active_indices,ones(2),nb_feat)\n",
    "    end\n",
    "end\n",
    "\n",
    "function visualize(m::InvertedPendulumModel,s::State,a::Action)\n",
    "    #NOTE: th = 0 is upright\n",
    "    th = s[1] + pi/2.\n",
    "    #base grid\n",
    "    w = 1.5*m.l\n",
    "    fill([-w,w,w,-w],[-w,-w,w,w],color=\"#FFFFFF\",edgecolor=\"#000000\")\n",
    "    #draw cart\n",
    "    dx = 0.05*sign(a)\n",
    "    h = 0.1\n",
    "    l = 0.125\n",
    "    fill([-l+dx;l+dx;l+dx;-l+dx],[-h;-h;h;h],color=\"#FF0000\")\n",
    "    #draw pole\n",
    "    u = m.l*cos(th) #+ dx\n",
    "    v = m.l*sin(th)\n",
    "    arrow(dx,0,u,v,width=m.l/10,head_width=0.,head_length=0.,color=\"#00FF00\")\n",
    "    #add cart direction (force)\n",
    "    if abs(dx) > 0\n",
    "        arrow(dx,0.,dx,0.,width=h,head_width=1.75*h,head_length=abs(dx),color=\"#0000FF\")\n",
    "    end\n",
    "    #add pole velocity \n",
    "    du = -s[2]*m.l*sin(th)/5.\n",
    "    dv = s[2]*m.l*cos(th)/5.\n",
    "    arrow(u,v,du,dv,width=m.l/10,head_width=m.l/5,head_length=m.l/5,color=\"#FF00FF\")\n",
    "end\n",
    "\n",
    "function visualize(m::InvertedPendulumModel,S::Array{State,1},A::Array{Action,1})\n",
    "  assert(length(S) == length(A))\n",
    "  f = figure()\n",
    "  @manipulate for i = 1:length(S); withfig(f) do\n",
    "    visualize(m,S[i],A[i]) end\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_A = Action[-50;0;50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = InvertedPendulumModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the BlackBoxModel type. Note that we do not include an observation function in the constructor--in this case, it uses a default identity observation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bbm = BlackBoxModel(m,init,next,reward,isend) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Policy\n",
    "\n",
    "In general for a policy, we have to define an ActionSpace (which we require to be exactly or a subset of the true action space), and feature function, which maps the state into a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tile coding is provided (the API for tilecoding needs work, however) for a quick and dirty function approximator in the continuous domain. For concreteness/generality, we include a function `cast_mc_state`, which in the most general case, will convert whatever state representation you have into an array of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_function = generate_joint_featurefunction(m)#generate_featurefunction(m,_A)\n",
    "#feature_function = generate_disjoint_featurefunction(m)\n",
    "A = DiscreteActionSpace(_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy = EpsilonGreedyPolicy(feature_function,A,rng=MersenneTwister(3234),eps=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose and Set up your Solver\n",
    "\n",
    "Currently, the following solvers are supported:\n",
    "+ Forgetful LSTD(\\lambda) / LS-SARSA (untested)\n",
    "+ SARSA(\\lamda) \n",
    "+ Q(\\lambda) (unimplemented)\n",
    "+ GQ(\\lambda) (unimplemented)\n",
    "+ Double Q learning (untested)\n",
    "+ Deterministic Policy Gradient (unimplemented)\n",
    "+ (Natural) Actor-Critic (unimplemented\n",
    "+ LSPI/Batch TD (untested)\n",
    "+ True Online TD\n",
    "\n",
    "We just ask that you know a-priori how big your feature vectors are to make initialization easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#there might be a smart way to stick this into a constructor, but for now...\n",
    "nb_features = length(ReinforcementLearning.expand(policy.exp,policy.feature_function(bbm.state),domain(A)[1]))\n",
    "updater = ForgetfulLSTDParam(nb_features,alpha=0.001/3)\n",
    "#updater = SARSAParam(nb_features,lambda=0.7,init_method=\"unif_rand\",trace_type=\"replacing\")\n",
    "updater = TrueOnlineTDParam(nb_features,lambda=0.95,init_method=\"unif_rand\")\n",
    "#mem_size = 50\n",
    "#updater = LSPIParam(nb_features,mem_size,del=0.01,discount=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually set up the real solver\n",
    "\n",
    "Some random cool things supported include:\n",
    "+ minibatching\n",
    "+ experience replay\n",
    "+ adaptive learning rates, e.g.:\n",
    "    * momentum\n",
    "    * nesterov momentum\n",
    "    * rmsprop\n",
    "    * adagrad\n",
    "    * adadelta\n",
    "    * adam\n",
    "+ simulated annealing (probably shouldn't support this)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solver = Solver(updater,\n",
    "                lr=0.01,\n",
    "                nb_episodes=500,\n",
    "                nb_timesteps=3000,\n",
    "                discount=0.99,\n",
    "                annealer=NullAnnealer(),\n",
    "                mb=NullMinibatcher(),\n",
    "                er=NullExperienceReplayer(),\n",
    "                display_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trained_policy = solve(solver,bbm,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Policy\n",
    "Basically just run a couple of simulations -- the simulator api is a subset of the stuff you see in solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sim = Simulator(discount=1.,nb_sim=50,nb_timesteps=250,visualizer=visualize) #stuff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#returns average reward for now...\n",
    "R_avg = simulate(sim,bbm,trained_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize(m,sim.hist.S,sim.hist.A)\n",
    "#note: joint features gets 8.82\n",
    "#note disjoint features gets: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have to call visualize externally. Currently getting the visualization to work two or three function calls in isn't quite working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.2",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
